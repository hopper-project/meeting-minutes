## 12/01/2015 Minutes:
* Matt updated the current progress of PTM:
  * setting: fix topics to MLE of wikipedia stat-th paper and run posterior inference on arxiv with fixed topics
  * three changes to clean this up:
    * before, was only doing 25 sweeps. compared this to 1000 sweeps and the posterior means are quite different. log-likelihood changes significantly from 25 to 200.
    * truncated arxiv documents to 1000 words
    * pruned wiki articles, removing articles that were too broad or too short
  * identified two salient issues we need to address:
    * we need to broaden our list of concepts. 
    * we need to account for the wiki 'background topics'
  * it would be nice to have a set of prototype arxiv documents which we consistently test on, to track incremental changes

* brainstorming:
  * coming up with a baseline metric to compare with PTM posterior topic distribution for arxiv documents:
    * create common vocabulary (perhaps consider adding bigrams)
    * compute TFIDF scores for each (word,document) pair, then for each arxiv document, A:
      * construct vocabulary vector for A, where each coordinate is equal to the TFIDF score 
      * construct similar TFIDF vector for wiki documents
      * finally create a similarity vector where each coordinate is the cosine similarity between A and a wiki document
      * compare this vector with the posterior topic distribution for A
    * this idea bove can also be used to index the arxiv documents and used to construct a search engine where queries are concepts 
    * we can also use this to choose the important wikipedia documents to use in PTM

## To Do:
* implement the TFIDF similarity ranking described above in MeTA
* derive collapsed gibbs sampler and update LDA code to account for background topics
* literature review: 
  * beyond unigram topic models?
  * turbo topics
